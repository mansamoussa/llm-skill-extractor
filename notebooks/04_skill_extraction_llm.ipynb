{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZrsr06wvqM6"
      },
      "source": [
        "# Multi-Stage Job Advertisement Analysis ‚Äî LLM Skill Extractor\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mansamoussa/llm-skill-extractor/blob/main/notebooks/04_skill_extraction_llm.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "### Objective\n",
        "Extract professional skills from job advertisements using a Large Language Model (OpenAI GPT) after identifying the relevant \"Skills and Content\" zones with our fine-tuned BERT model.\n",
        "\n",
        "This notebook will:\n",
        "1. Load the fine-tuned BERT model from Task 2 (zone identification)\n",
        "2. Load job advertisement data (both annotated and scraped datasets)\n",
        "3. Use BERT to identify \"F√§higkeiten und Inhalte\" (Skills and Content) zones\n",
        "4. Extract those text sections containing skills\n",
        "5. Use OpenAI GPT API to intelligently extract individual professional skills\n",
        "6. Structure and save the extracted skills in JSON format\n",
        "7. Evaluate the quality and coverage of skill extraction\n",
        "\n",
        "### Why This Task is Important\n",
        "The BERT model from Tasks 1-3 can identify WHERE skills are mentioned in a job ad, but it cannot extract the SPECIFIC skills themselves. This is a perfect task for a Large Language Model (LLM) because:\n",
        "- LLMs understand context and can identify implicit skills\n",
        "- They can handle multilingual text (German, French, English)\n",
        "- They can normalize skill names (e.g., \"JS\" ‚Üí \"JavaScript\")\n",
        "- They can distinguish between skills and other job requirements\n",
        "\n",
        "### Input Data\n",
        "- `model/best_model.pt` ‚Äî fine-tuned BERT model from Task 2\n",
        "- `model/id2label.json` and `model/label2id.json` ‚Äî label mappings\n",
        "- `data/annotated.json` ‚Äî annotated job advertisements (German/French)\n",
        "- `data/scraped/remoteok_jobs.jsonl` ‚Äî scraped job postings (English)\n",
        "\n",
        "### Output\n",
        "- `data/extracted_skills.json` ‚Äî structured skill extraction results\n",
        "- `data/skill_statistics.json` ‚Äî statistics and analysis\n",
        "- Evaluation metrics and visualizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. Setting up my Environment\n",
        "\n",
        "**Objective:** Prepare the Google Colab environment by cloning the project repository and installing necessary dependencies.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** A fresh Colab session is empty and doesn't have my project files or the required libraries.\n",
        "* **The Fix:** I clone my repository to get the code, and install dependencies including OpenAI API library.\n",
        "\n",
        "**What I did:**\n",
        "1. **Clone Repository:** Downloaded project files from GitHub\n",
        "2. **Install Dependencies:** Installed Python libraries including `openai`, `torch`, `transformers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- SETUP STEPS ---\n",
        "# This cell prepares the Colab environment by downloading the code and installing libraries.\n",
        "import os\n",
        "\n",
        "# 1. Clone the repository if it doesn't exist in the notebook\n",
        "if not os.path.exists('llm-skill-extractor'):\n",
        "    !git clone https://github.com/mansamoussa/llm-skill-extractor.git\n",
        "else:\n",
        "    print(\"Repository already cloned.\")\n",
        "\n",
        "# 2. Install core dependencies (including both OpenAI and Google Gemini)\n",
        "!pip install -q torch transformers openai google-generativeai pandas tqdm scikit-learn matplotlib seaborn\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Loading my Data\n",
        "\n",
        "**Objective:** Load both the annotated dataset and scraped job postings from Google Drive.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** The annotated data is private and not in the GitHub repo.\n",
        "* **The Fix:** I mount Google Drive and copy the necessary data files to the project folder.\n",
        "\n",
        "**What I did:**\n",
        "1. **Mount Drive:** Connected to Google Drive\n",
        "2. **Copy Files:** Copied `annotated.json` to the data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- DATA LOADING STEP ---\n",
        "# This cell brings the data from Google Drive into the project environment\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define paths (adjust if your Drive structure is different)\n",
        "source_path = '/content/drive/MyDrive/GEN03/annotated.json'\n",
        "destination_folder = '/content/llm-skill-extractor/data/'\n",
        "\n",
        "# 3. Copy the annotated data file\n",
        "if os.path.exists(source_path):\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "    shutil.copy(source_path, destination_folder)\n",
        "    print(f\"‚úÖ Success! annotated.json copied to {destination_folder}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è File not found at {source_path}\")\n",
        "    print(\"Please verify the path or upload 'annotated.json' manually to 'llm-skill-extractor/data/' folder.\")\n",
        "\n",
        "print(\"\\nüìä Checking data files...\")\n",
        "data_files = ['annotated.json', 'scraped/remoteok_jobs.jsonl']\n",
        "for file in data_files:\n",
        "    full_path = os.path.join(destination_folder, file)\n",
        "    if os.path.exists(full_path):\n",
        "        size_mb = os.path.getsize(full_path) / (1024 * 1024)\n",
        "        print(f\"‚úÖ Found: {file} ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå Missing: {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Loading the Trained BERT Model\n",
        "\n",
        "**Objective:** Load the fine-tuned BERT model and label mappings from Task 2 (or from Google Drive backup).\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** I need the trained model to identify which parts of the job ads contain skills.\n",
        "* **The Fix:** Load the model weights, tokenizer, and label mappings from the previous training task.\n",
        "\n",
        "**What I did:**\n",
        "1. **Check Local Files:** Look for model files in the project folder\n",
        "2. **Load from Drive:** If not found locally, copy from Google Drive backup\n",
        "3. **Initialize Model:** Load BERT model with trained weights\n",
        "4. **Load Mappings:** Load the label2id and id2label dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- MODEL LOADING STEP ---\n",
        "# Load the trained BERT model for zone identification\n",
        "import torch\n",
        "import json\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "\n",
        "# Define paths\n",
        "PROJECT_ROOT = '/content/llm-skill-extractor'\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, 'model')\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, 'best_model.pt')\n",
        "ID2LABEL_PATH = os.path.join(MODEL_DIR, 'id2label.json')\n",
        "LABEL2ID_PATH = os.path.join(MODEL_DIR, 'label2id.json')\n",
        "\n",
        "# Check if model files exist locally, if not try to load from Drive\n",
        "drive_model_path = '/content/drive/MyDrive/GEN03/model'\n",
        "if not os.path.exists(BEST_MODEL_PATH) and os.path.exists(drive_model_path):\n",
        "    print(\"üì• Model not found locally, copying from Google Drive...\")\n",
        "    shutil.copytree(drive_model_path, MODEL_DIR, dirs_exist_ok=True)\n",
        "    print(\"‚úÖ Model files copied from Drive\")\n",
        "\n",
        "# Load label mappings\n",
        "print(\"üìñ Loading label mappings...\")\n",
        "with open(ID2LABEL_PATH, 'r') as f:\n",
        "    id2label = json.load(f)\n",
        "    # Convert keys to integers\n",
        "    id2label = {int(k): v for k, v in id2label.items()}\n",
        "\n",
        "with open(LABEL2ID_PATH, 'r') as f:\n",
        "    label2id = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Found {len(label2id)} labels: {list(label2id.keys())}\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(\"üî§ Loading tokenizer...\")\n",
        "MODEL_NAME = 'bert-base-multilingual-cased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Initialize and load the model\n",
        "print(\"ü§ñ Loading BERT model...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_labels = len(label2id)\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "checkpoint = torch.load(BEST_MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ BERT model loaded successfully!\")\n",
        "print(f\"   Model has {num_labels} output labels\")\n",
        "print(f\"   Target label for skill extraction: 'F√§higkeiten und Inhalte'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Configuring LLM APIs (OpenAI & Gemini)\n",
        "\n",
        "**Objective:** Set up both OpenAI and Google Gemini API connections for skill extraction.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** I need to authenticate with LLM providers to use their models for intelligent skill extraction.\n",
        "* **The Fix:** Configure both API keys and allow choosing which model to use.\n",
        "\n",
        "**Why Both APIs?**\n",
        "* **Flexibility:** Choose the best model for your needs\n",
        "* **Comparison:** Test which LLM performs better for skill extraction  \n",
        "* **Cost:** Gemini is free, OpenAI costs money but might be more accurate\n",
        "\n",
        "**Security Note:** In production, never hardcode API keys. Use environment variables or secret management systems.\n",
        "\n",
        "**What I did:**\n",
        "1. **Set API Keys:** Configure both OpenAI and Gemini authentication\n",
        "2. **Test Connections:** Verify both APIs are accessible\n",
        "3. **Choose Default:** Select which LLM to use (can be changed later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- LLM SETUP (OPENAI & GEMINI) ---\n",
        "# Configure both OpenAI and Google Gemini APIs for skill extraction\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Set your API keys  \n",
        "# IMPORTANT: Replace with your own API keys before running\n",
        "# Option 1: Hardcode them here (not recommended for public repos)\n",
        "# Option 2: Use Google Colab secrets (recommended)\n",
        "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY_HERE\"  # Replace with your OpenAI API key\n",
        "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"   # Replace with your Gemini API key\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Test OpenAI connection\n",
        "print(\"üîó Testing OpenAI API connection...\")\n",
        "try:\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Say 'OpenAI API test successful' in 5 words or less.\"}\n",
        "        ],\n",
        "        max_tokens=20\n",
        "    )\n",
        "    print(f\"‚úÖ OpenAI API Connection successful!\")\n",
        "    print(f\"   Response: {response.choices[0].message.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API Connection failed: {e}\")\n",
        "    openai_client = None\n",
        "\n",
        "# Test Gemini connection\n",
        "print(\"\\nüîó Testing Gemini API connection...\")\n",
        "try:\n",
        "    response = gemini_model.generate_content(\"Say 'Gemini API test successful' in 5 words or less.\")\n",
        "    print(f\"‚úÖ Gemini API Connection successful!\")\n",
        "    print(f\"   Response: {response.text.strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Gemini API Connection failed: {e}\")\n",
        "    gemini_model = None\n",
        "\n",
        "# Choose which LLM to use (you can change this later)\n",
        "# Options: \"openai\" or \"gemini\"\n",
        "USE_LLM = \"gemini\"  # Default to Gemini (free tier)\n",
        "\n",
        "print(f\"\\nüéØ Selected LLM: {USE_LLM.upper()}\")\n",
        "if USE_LLM == \"openai\" and openai_client is None:\n",
        "    print(\"‚ö†Ô∏è OpenAI selected but connection failed. Falling back to Gemini.\")\n",
        "    USE_LLM = \"gemini\"\n",
        "elif USE_LLM == \"gemini\" and gemini_model is None:\n",
        "    print(\"‚ö†Ô∏è Gemini selected but connection failed. Falling back to OpenAI.\")\n",
        "    USE_LLM = \"openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Helper Function: Extract Skills Section Using BERT\n",
        "\n",
        "**Objective:** Create a function that uses the trained BERT model to identify and extract the \"F√§higkeiten und Inhalte\" (Skills and Content) sections from job advertisements.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Job ads contain many sections (company info, benefits, application process, etc.). I only want the parts that list skills.\n",
        "* **The Fix:** Use BERT to classify each token, then extract continuous spans labeled as \"F√§higkeiten und Inhalte\".\n",
        "\n",
        "**How it works:**\n",
        "1. Tokenize the job ad text\n",
        "2. Run BERT to predict labels for each token\n",
        "3. Find all tokens labeled as \"F√§higkeiten und Inhalte\"\n",
        "4. Reconstruct the original text from those tokens\n",
        "5. Return the extracted skill sections\n",
        "\n",
        "**What I did:**\n",
        "Created a reusable function that processes any job advertisement text and returns only the skills sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- BERT EXTRACTION FUNCTION ---\n",
        "# Function to extract skills sections using the trained BERT model\n",
        "\n",
        "def extract_skills_section_with_bert(text, model, tokenizer, device, target_label=\"F√§higkeiten und Inhalte\"):\n",
        "    \"\"\"\n",
        "    Extract sections labeled as target_label from the input text using BERT.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input job advertisement text\n",
        "        model: Trained BERT model for token classification\n",
        "        tokenizer: BERT tokenizer\n",
        "        device: torch device (cuda or cpu)\n",
        "        target_label (str): The label to extract (default: \"F√§higkeiten und Inhalte\")\n",
        "    \n",
        "    Returns:\n",
        "        list: List of extracted text sections labeled as target_label\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Tokenize with offset mapping to track character positions\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "    \n",
        "    offset_mapping = encoding.pop('offset_mapping')[0]\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)[0]\n",
        "    \n",
        "    # Convert predictions to labels\n",
        "    predicted_labels = [id2label[pred.item()] for pred in predictions]\n",
        "    \n",
        "    # Extract skill sections\n",
        "    skill_sections = []\n",
        "    current_section = []\n",
        "    current_start = None\n",
        "    \n",
        "    for idx, (label, (start, end)) in enumerate(zip(predicted_labels, offset_mapping)):\n",
        "        # Skip special tokens ([CLS], [SEP], [PAD])\n",
        "        if start == end == 0:\n",
        "            continue\n",
        "            \n",
        "        if label == target_label:\n",
        "            if current_start is None:\n",
        "                current_start = start.item()\n",
        "            current_section.append((start.item(), end.item()))\n",
        "        else:\n",
        "            # End of a skill section\n",
        "            if current_section:\n",
        "                # Extract the text for this section\n",
        "                section_start = current_section[0][0]\n",
        "                section_end = current_section[-1][1]\n",
        "                section_text = text[section_start:section_end].strip()\n",
        "                if section_text:\n",
        "                    skill_sections.append(section_text)\n",
        "                current_section = []\n",
        "                current_start = None\n",
        "    \n",
        "    # Don't forget the last section if it ends at the document end\n",
        "    if current_section:\n",
        "        section_start = current_section[0][0]\n",
        "        section_end = current_section[-1][1]\n",
        "        section_text = text[section_start:section_end].strip()\n",
        "        if section_text:\n",
        "            skill_sections.append(section_text)\n",
        "    \n",
        "    return skill_sections\n",
        "\n",
        "print(\"‚úÖ BERT extraction function defined!\")\n",
        "print(\"   Function: extract_skills_section_with_bert()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Helper Function: Extract Skills Using LLM\n",
        "\n",
        "**Objective:** Create a function that uses OpenAI GPT to extract individual skills from the text sections identified by BERT.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** The BERT model tells me WHERE skills are mentioned, but not WHAT the specific skills are.\n",
        "* **The Fix:** Use an LLM with a carefully crafted prompt to intelligently extract and normalize skill names.\n",
        "\n",
        "**How it works:**\n",
        "1. Take a text section containing skills\n",
        "2. Send it to GPT with a specialized prompt\n",
        "3. GPT identifies individual skills (technical skills, soft skills, languages, tools)\n",
        "4. Return structured JSON with categorized skills\n",
        "\n",
        "**Prompt Engineering:**\n",
        "The prompt instructs GPT to:\n",
        "- Extract only professional skills (not job titles or company names)\n",
        "- Normalize skill names (e.g., \"JS\" ‚Üí \"JavaScript\")\n",
        "- Categorize skills (technical, soft, languages, tools, certifications)\n",
        "- Return results in a structured JSON format\n",
        "- Handle multilingual text (German, French, English)\n",
        "\n",
        "**What I did:**\n",
        "Created a reusable function with a well-engineered prompt for consistent skill extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- LLM EXTRACTION FUNCTIONS ---\n",
        "# Functions to extract individual skills using OpenAI GPT or Google Gemini\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_skills_with_openai(text_section, client, model_name=\"gpt-4o-mini\"):\n",
        "    \"\"\"Extract skills using OpenAI GPT.\"\"\"\n",
        "    if not text_section or len(text_section.strip()) == 0:\n",
        "        return get_empty_skills_dict()\n",
        "    \n",
        "    system_prompt = get_skill_extraction_prompt()\n",
        "    user_prompt = f\"\"\"Extract and categorize all professional skills from this job advertisement text:\n",
        "\n",
        "TEXT:\n",
        "{text_section}\n",
        "\n",
        "Return only the JSON object with categorized skills.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=1000,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        \n",
        "        skills_data = json.loads(response.choices[0].message.content)\n",
        "        return ensure_all_skill_keys(skills_data)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è OpenAI extraction error: {e}\")\n",
        "        return get_empty_skills_dict(error=str(e))\n",
        "\n",
        "def extract_skills_with_gemini(text_section, model):\n",
        "    \"\"\"Extract skills using Google Gemini.\"\"\"\n",
        "    if not text_section or len(text_section.strip()) == 0:\n",
        "        return get_empty_skills_dict()\n",
        "    \n",
        "    system_prompt = get_skill_extraction_prompt()\n",
        "    full_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "Extract and categorize all professional skills from this job advertisement text:\n",
        "\n",
        "TEXT:\n",
        "{text_section}\n",
        "\n",
        "Return ONLY the JSON object with categorized skills, no other text.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=genai.GenerationConfig(\n",
        "                temperature=0.3,\n",
        "                max_output_tokens=1000,\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Extract JSON from response (Gemini might add markdown code blocks)\n",
        "        response_text = response.text.strip()\n",
        "        \n",
        "        # Remove markdown code blocks if present\n",
        "        json_match = re.search(r'```(?:json)?\\s*({.*?})\\s*```', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "        else:\n",
        "            # Try to find JSON object directly\n",
        "            json_match = re.search(r'{.*}', response_text, re.DOTALL)\n",
        "            json_str = json_match.group(0) if json_match else response_text\n",
        "        \n",
        "        skills_data = json.loads(json_str)\n",
        "        return ensure_all_skill_keys(skills_data)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Gemini extraction error: {e}\")\n",
        "        return get_empty_skills_dict(error=str(e))\n",
        "\n",
        "def extract_skills_with_llm(text_section, use_llm=\"gemini\"):\n",
        "    \"\"\"\n",
        "    Unified function to extract skills using either OpenAI or Gemini.\n",
        "    \n",
        "    Args:\n",
        "        text_section (str): Text containing skill descriptions\n",
        "        use_llm (str): Which LLM to use - \"openai\" or \"gemini\"\n",
        "    \n",
        "    Returns:\n",
        "        dict: Structured skills data with categories\n",
        "    \"\"\"\n",
        "    if use_llm == \"openai\" and openai_client:\n",
        "        return extract_skills_with_openai(text_section, openai_client)\n",
        "    elif use_llm == \"gemini\" and gemini_model:\n",
        "        return extract_skills_with_gemini(text_section, gemini_model)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Selected LLM '{use_llm}' not available\")\n",
        "        return get_empty_skills_dict(error=f\"LLM {use_llm} not available\")\n",
        "\n",
        "# Helper functions\n",
        "def get_empty_skills_dict(error=None):\n",
        "    \"\"\"Return an empty skills dictionary.\"\"\"\n",
        "    result = {\n",
        "        \"technical_skills\": [],\n",
        "        \"soft_skills\": [],\n",
        "        \"languages\": [],\n",
        "        \"tools\": [],\n",
        "        \"certifications\": [],\n",
        "        \"other_skills\": []\n",
        "    }\n",
        "    if error:\n",
        "        result[\"error\"] = error\n",
        "    return result\n",
        "\n",
        "def ensure_all_skill_keys(skills_data):\n",
        "    \"\"\"Ensure all expected keys exist in the skills dictionary.\"\"\"\n",
        "    expected_keys = [\"technical_skills\", \"soft_skills\", \"languages\", \"tools\", \"certifications\", \"other_skills\"]\n",
        "    for key in expected_keys:\n",
        "        if key not in skills_data:\n",
        "            skills_data[key] = []\n",
        "    return skills_data\n",
        "\n",
        "def get_skill_extraction_prompt():\n",
        "    \"\"\"Get the standard prompt for skill extraction.\"\"\"\n",
        "    return \"\"\"You are an expert HR analyst specializing in extracting professional skills from job advertisements.\n",
        "Your task is to extract and categorize skills mentioned in the provided text.\n",
        "\n",
        "RULES:\n",
        "1. Extract only genuine professional skills, tools, technologies, and qualifications\n",
        "2. Do NOT extract:\n",
        "   - Job titles or positions\n",
        "   - Company names\n",
        "   - General job responsibilities\n",
        "   - Benefits or salary information\n",
        "3. Normalize skill names (e.g., \"JS\" ‚Üí \"JavaScript\", \"ML\" ‚Üí \"Machine Learning\")\n",
        "4. Handle multilingual text (German, French, English)\n",
        "5. Categorize skills appropriately\n",
        "\n",
        "Return ONLY a JSON object with this exact structure (no additional text):\n",
        "{\n",
        "  \"technical_skills\": [\"skill1\", \"skill2\"],\n",
        "  \"soft_skills\": [\"skill1\", \"skill2\"],\n",
        "  \"languages\": [\"language1\", \"language2\"],\n",
        "  \"tools\": [\"tool1\", \"tool2\"],\n",
        "  \"certifications\": [\"cert1\", \"cert2\"],\n",
        "  \"other_skills\": [\"skill1\", \"skill2\"]\n",
        "}\"\"\"\n",
        "\n",
        "print(\"‚úÖ LLM extraction functions defined!\")\n",
        "print(f\"   Functions: extract_skills_with_llm() [supports both OpenAI and Gemini]\")\n",
        "print(f\"   Currently using: {USE_LLM.upper()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Load and Prepare Job Advertisement Data\n",
        "\n",
        "**Objective:** Load both the annotated dataset and scraped job postings, preparing them for skill extraction.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** I have two different data sources with different formats.\n",
        "* **The Fix:** Load both datasets and standardize them into a common format.\n",
        "\n",
        "**What I did:**\n",
        "1. Load annotated.json (German/French job ads)\n",
        "2. Load scraped remoteok_jobs.jsonl (English job ads)\n",
        "3. Create a unified data structure\n",
        "4. Display sample data for verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- DATA LOADING ---\n",
        "# Load and prepare job advertisement data\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n",
        "\n",
        "# 1. Load annotated data (German/French)\n",
        "print(\"üìñ Loading annotated job advertisements...\")\n",
        "annotated_path = os.path.join(DATA_PATH, 'annotated.json')\n",
        "df_annotated = pd.read_json(annotated_path)\n",
        "\n",
        "# Extract the text content from the data field\n",
        "df_annotated['text'] = df_annotated['data'].apply(lambda x: x.get('content_clean', '') if isinstance(x, dict) else '')\n",
        "df_annotated['source'] = 'annotated'\n",
        "df_annotated['language'] = 'de/fr'  # German/French\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df_annotated)} annotated job ads\")\n",
        "\n",
        "# 2. Load scraped data (English)\n",
        "print(\"üìñ Loading scraped job advertisements...\")\n",
        "scraped_path = os.path.join(DATA_PATH, 'scraped', 'remoteok_jobs.jsonl')\n",
        "\n",
        "scraped_jobs = []\n",
        "with open(scraped_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            scraped_jobs.append(json.loads(line))\n",
        "\n",
        "df_scraped = pd.DataFrame(scraped_jobs)\n",
        "df_scraped['text'] = df_scraped['description']\n",
        "df_scraped['source'] = 'scraped'\n",
        "df_scraped['language'] = 'en'\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df_scraped)} scraped job ads\")\n",
        "\n",
        "# 3. Create unified dataset\n",
        "print(\"\\nüîÑ Creating unified dataset...\")\n",
        "jobs_data = []\n",
        "\n",
        "# Add annotated jobs\n",
        "for idx, row in df_annotated.iterrows():\n",
        "    jobs_data.append({\n",
        "        'id': f\"annotated_{idx}\",\n",
        "        'text': row['text'],\n",
        "        'source': row['source'],\n",
        "        'language': row['language'],\n",
        "        'title': '',  # Not available in annotated data\n",
        "        'company': ''  # Not available in annotated data\n",
        "    })\n",
        "\n",
        "# Add scraped jobs\n",
        "for idx, row in df_scraped.iterrows():\n",
        "    jobs_data.append({\n",
        "        'id': f\"scraped_{idx}\",\n",
        "        'text': row['text'],\n",
        "        'source': row['source'],\n",
        "        'language': row['language'],\n",
        "        'title': row.get('title', ''),\n",
        "        'company': row.get('company', '')\n",
        "    })\n",
        "\n",
        "print(f\"‚úÖ Total dataset: {len(jobs_data)} job advertisements\")\n",
        "print(f\"   - Annotated (de/fr): {len(df_annotated)}\")\n",
        "print(f\"   - Scraped (en): {len(df_scraped)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nüìä Sample job advertisement:\")\n",
        "sample = jobs_data[0]\n",
        "print(f\"ID: {sample['id']}\")\n",
        "print(f\"Source: {sample['source']}\")\n",
        "print(f\"Language: {sample['language']}\")\n",
        "print(f\"Text preview: {sample['text'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Test Extraction Pipeline on Sample\n",
        "\n",
        "**Objective:** Test the complete extraction pipeline (BERT + LLM) on a sample job advertisement before processing the full dataset.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Processing all job ads with the API costs money and time. I need to verify the pipeline works correctly first.\n",
        "* **The Fix:** Run the pipeline on a few sample ads and inspect the results.\n",
        "\n",
        "**What I did:**\n",
        "1. Select a sample job advertisement\n",
        "2. Extract skills sections using BERT\n",
        "3. Extract individual skills using LLM\n",
        "4. Display and verify the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- PIPELINE TEST ---\n",
        "# Test the extraction pipeline on sample data\n",
        "\n",
        "print(\"üß™ Testing extraction pipeline on sample job advertisement...\\n\")\n",
        "\n",
        "# Select a sample from scraped data (usually has clearer skill descriptions)\n",
        "test_sample = [job for job in jobs_data if job['source'] == 'scraped'][0]\n",
        "\n",
        "print(f\"üìÑ Sample Job: {test_sample['title']}\")\n",
        "print(f\"   Company: {test_sample['company']}\")\n",
        "print(f\"   Source: {test_sample['source']}\")\n",
        "print(f\"   Text length: {len(test_sample['text'])} characters\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Step 1: Extract skills sections using BERT\n",
        "print(\"\\nü§ñ Step 1: Extracting skills sections with BERT...\")\n",
        "skill_sections = extract_skills_section_with_bert(\n",
        "    test_sample['text'],\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Found {len(skill_sections)} skills section(s)\")\n",
        "for i, section in enumerate(skill_sections, 1):\n",
        "    print(f\"\\n   Section {i} ({len(section)} chars):\")\n",
        "    print(f\"   {section[:200]}...\" if len(section) > 200 else f\"   {section}\")\n",
        "\n",
        "# Step 2: Extract individual skills using LLM\n",
        "print(\"\\nüß† Step 2: Extracting individual skills with LLM...\")\n",
        "all_extracted_skills = {\n",
        "    \"technical_skills\": [],\n",
        "    \"soft_skills\": [],\n",
        "    \"languages\": [],\n",
        "    \"tools\": [],\n",
        "    \"certifications\": [],\n",
        "    \"other_skills\": []\n",
        "}\n",
        "\n",
        "for i, section in enumerate(skill_sections, 1):\n",
        "    print(f\"\\n   Processing section {i}...\")\n",
        "    skills = extract_skills_with_llm(section, use_llm=USE_LLM)\n",
        "    \n",
        "    # Merge results\n",
        "    for key in all_extracted_skills.keys():\n",
        "        if key in skills:\n",
        "            all_extracted_skills[key].extend(skills[key])\n",
        "\n",
        "# Remove duplicates\n",
        "for key in all_extracted_skills.keys():\n",
        "    all_extracted_skills[key] = list(set(all_extracted_skills[key]))\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä EXTRACTION RESULTS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for category, skills in all_extracted_skills.items():\n",
        "    if skills:\n",
        "        print(f\"\\n{category.upper().replace('_', ' ')}:\")\n",
        "        for skill in sorted(skills):\n",
        "            print(f\"  ‚Ä¢ {skill}\")\n",
        "\n",
        "total_skills = sum(len(v) for v in all_extracted_skills.values())\n",
        "print(f\"\\n‚úÖ Total unique skills extracted: {total_skills}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Process Full Dataset\n",
        "\n",
        "**Objective:** Process all job advertisements through the extraction pipeline and save results.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** I need to extract skills from all job ads, not just samples.\n",
        "* **The Fix:** Loop through all job ads, extract skills, save progress regularly.\n",
        "\n",
        "**What I did:**\n",
        "1. Process each job advertisement through the pipeline\n",
        "2. Save progress every 10 jobs (in case of errors)\n",
        "3. Display progress and statistics\n",
        "4. Handle errors gracefully\n",
        "\n",
        "**Note:** This cell may take 10-30 minutes depending on dataset size and API speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- FULL DATASET PROCESSING ---\n",
        "# Process all job advertisements and extract skills\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"üöÄ Starting full dataset processing...\")\n",
        "print(f\"   Total jobs to process: {len(jobs_data)}\")\n",
        "print(f\"   Estimated time: ~{len(jobs_data) * 3 / 60:.1f} minutes\\n\")\n",
        "\n",
        "# Initialize results storage\n",
        "extraction_results = []\n",
        "errors = []\n",
        "\n",
        "# Process each job\n",
        "for i, job in enumerate(tqdm(jobs_data, desc=\"Extracting skills\")):\n",
        "    try:\n",
        "        # Extract skills sections with BERT\n",
        "        skill_sections = extract_skills_section_with_bert(\n",
        "            job['text'],\n",
        "            model,\n",
        "            tokenizer,\n",
        "            device\n",
        "        )\n",
        "        \n",
        "        # Extract individual skills with LLM\n",
        "        all_skills = {\n",
        "            \"technical_skills\": [],\n",
        "            \"soft_skills\": [],\n",
        "            \"languages\": [],\n",
        "            \"tools\": [],\n",
        "            \"certifications\": [],\n",
        "            \"other_skills\": []\n",
        "        }\n",
        "        \n",
        "        for section in skill_sections:\n",
        "            if section:  # Only process non-empty sections\n",
        "                skills = extract_skills_with_llm(section, use_llm=USE_LLM)\n",
        "                \n",
        "                # Merge results\n",
        "                for key in all_skills.keys():\n",
        "                    if key in skills:\n",
        "                        all_skills[key].extend(skills[key])\n",
        "        \n",
        "        # Remove duplicates\n",
        "        for key in all_skills.keys():\n",
        "            all_skills[key] = list(set(all_skills[key]))\n",
        "        \n",
        "        # Store result\n",
        "        result = {\n",
        "            'job_id': job['id'],\n",
        "            'source': job['source'],\n",
        "            'language': job['language'],\n",
        "            'title': job.get('title', ''),\n",
        "            'company': job.get('company', ''),\n",
        "            'num_skill_sections': len(skill_sections),\n",
        "            'extracted_skills': all_skills,\n",
        "            'total_skills_count': sum(len(v) for v in all_skills.values())\n",
        "        }\n",
        "        extraction_results.append(result)\n",
        "        \n",
        "        # Rate limiting: small delay to avoid hitting API limits\n",
        "        time.sleep(0.5)\n",
        "        \n",
        "        # Save intermediate results every 10 jobs\n",
        "        if (i + 1) % 10 == 0:\n",
        "            intermediate_path = os.path.join(DATA_PATH, f'extracted_skills_checkpoint_{i+1}.json')\n",
        "            with open(intermediate_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(extraction_results, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_info = {\n",
        "            'job_id': job['id'],\n",
        "            'error': str(e),\n",
        "            'index': i\n",
        "        }\n",
        "        errors.append(error_info)\n",
        "        print(f\"\\n‚ö†Ô∏è Error processing job {job['id']}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n‚úÖ Processing complete!\")\n",
        "print(f\"   Successfully processed: {len(extraction_results)} jobs\")\n",
        "print(f\"   Errors encountered: {len(errors)} jobs\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n‚ö†Ô∏è Jobs with errors:\")\n",
        "    for err in errors[:5]:  # Show first 5 errors\n",
        "        print(f\"   - {err['job_id']}: {err['error'][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Save Extraction Results\n",
        "\n",
        "**Objective:** Save the extracted skills data to JSON files for further analysis.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** The extraction results are in memory and will be lost when the session ends.\n",
        "* **The Fix:** Save results to structured JSON files.\n",
        "\n",
        "**What I did:**\n",
        "1. Save complete extraction results\n",
        "2. Save error log (if any)\n",
        "3. Create a summary statistics file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- SAVE RESULTS ---\n",
        "# Save extraction results to JSON files\n",
        "\n",
        "import json\n",
        "\n",
        "# 1. Save main extraction results\n",
        "output_path = os.path.join(DATA_PATH, 'extracted_skills.json')\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(extraction_results, f, indent=2, ensure_ascii=False)\n",
        "print(f\"üíæ Saved extraction results to: {output_path}\")\n",
        "\n",
        "# 2. Save errors (if any)\n",
        "if errors:\n",
        "    errors_path = os.path.join(DATA_PATH, 'extraction_errors.json')\n",
        "    with open(errors_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(errors, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"üíæ Saved error log to: {errors_path}\")\n",
        "\n",
        "# 3. Create and save statistics\n",
        "stats = {\n",
        "    'total_jobs_processed': len(extraction_results),\n",
        "    'total_errors': len(errors),\n",
        "    'jobs_by_source': {},\n",
        "    'jobs_by_language': {},\n",
        "    'total_skills_by_category': {\n",
        "        'technical_skills': 0,\n",
        "        'soft_skills': 0,\n",
        "        'languages': 0,\n",
        "        'tools': 0,\n",
        "        'certifications': 0,\n",
        "        'other_skills': 0\n",
        "    },\n",
        "    'average_skills_per_job': 0,\n",
        "    'jobs_with_no_skills': 0\n",
        "}\n",
        "\n",
        "# Calculate statistics\n",
        "for result in extraction_results:\n",
        "    # By source\n",
        "    source = result['source']\n",
        "    stats['jobs_by_source'][source] = stats['jobs_by_source'].get(source, 0) + 1\n",
        "    \n",
        "    # By language\n",
        "    lang = result['language']\n",
        "    stats['jobs_by_language'][lang] = stats['jobs_by_language'].get(lang, 0) + 1\n",
        "    \n",
        "    # Skills counts\n",
        "    for category, skills in result['extracted_skills'].items():\n",
        "        stats['total_skills_by_category'][category] += len(skills)\n",
        "    \n",
        "    # Jobs with no skills\n",
        "    if result['total_skills_count'] == 0:\n",
        "        stats['jobs_with_no_skills'] += 1\n",
        "\n",
        "# Average skills per job\n",
        "total_skills = sum(result['total_skills_count'] for result in extraction_results)\n",
        "stats['average_skills_per_job'] = total_skills / len(extraction_results) if extraction_results else 0\n",
        "\n",
        "# Save statistics\n",
        "stats_path = os.path.join(DATA_PATH, 'skill_statistics.json')\n",
        "with open(stats_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "print(f\"üíæ Saved statistics to: {stats_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ All results saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Analysis and Visualization\n",
        "\n",
        "**Objective:** Analyze and visualize the extracted skills data to understand patterns and quality.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Raw extraction results are hard to interpret.\n",
        "* **The Fix:** Create visualizations and summary statistics.\n",
        "\n",
        "**What I did:**\n",
        "1. Display key statistics\n",
        "2. Show most common skills by category\n",
        "3. Create visualizations\n",
        "4. Analyze skill distribution across sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- ANALYSIS AND VISUALIZATION ---\n",
        "# Analyze and visualize the extracted skills\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"üìä EXTRACTION STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"  ‚Ä¢ Total jobs processed: {stats['total_jobs_processed']}\")\n",
        "print(f\"  ‚Ä¢ Jobs with errors: {stats['total_errors']}\")\n",
        "print(f\"  ‚Ä¢ Jobs with no skills found: {stats['jobs_with_no_skills']}\")\n",
        "print(f\"  ‚Ä¢ Average skills per job: {stats['average_skills_per_job']:.2f}\")\n",
        "\n",
        "print(f\"\\nJobs by Source:\")\n",
        "for source, count in stats['jobs_by_source'].items():\n",
        "    print(f\"  ‚Ä¢ {source}: {count}\")\n",
        "\n",
        "print(f\"\\nJobs by Language:\")\n",
        "for lang, count in stats['jobs_by_language'].items():\n",
        "    print(f\"  ‚Ä¢ {lang}: {count}\")\n",
        "\n",
        "print(f\"\\nTotal Skills by Category:\")\n",
        "for category, count in stats['total_skills_by_category'].items():\n",
        "    print(f\"  ‚Ä¢ {category.replace('_', ' ').title()}: {count}\")\n",
        "\n",
        "# Collect all skills by category for frequency analysis\n",
        "all_skills_by_category = {\n",
        "    'technical_skills': [],\n",
        "    'soft_skills': [],\n",
        "    'languages': [],\n",
        "    'tools': [],\n",
        "    'certifications': [],\n",
        "    'other_skills': []\n",
        "}\n",
        "\n",
        "for result in extraction_results:\n",
        "    for category, skills in result['extracted_skills'].items():\n",
        "        all_skills_by_category[category].extend(skills)\n",
        "\n",
        "# Show top skills in each category\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ TOP SKILLS BY CATEGORY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for category, skills in all_skills_by_category.items():\n",
        "    if skills:\n",
        "        skill_counts = Counter(skills)\n",
        "        top_10 = skill_counts.most_common(10)\n",
        "        print(f\"\\n{category.replace('_', ' ').upper()}:\")\n",
        "        for skill, count in top_10:\n",
        "            print(f\"  {count:3d}x  {skill}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "\n",
        "# 1. Skills per job distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Distribution of total skills per job\n",
        "skills_counts = [result['total_skills_count'] for result in extraction_results]\n",
        "axes[0, 0].hist(skills_counts, bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Distribution of Total Skills per Job', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Number of Skills')\n",
        "axes[0, 0].set_ylabel('Number of Jobs')\n",
        "axes[0, 0].axvline(stats['average_skills_per_job'], color='red', linestyle='--', label=f'Average: {stats[\"average_skills_per_job\"]:.1f}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Plot 2: Skills by category\n",
        "categories = list(stats['total_skills_by_category'].keys())\n",
        "counts = list(stats['total_skills_by_category'].values())\n",
        "axes[0, 1].barh(categories, counts, color='skyblue', edgecolor='black')\n",
        "axes[0, 1].set_title('Total Skills by Category', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Number of Skills')\n",
        "axes[0, 1].set_ylabel('Category')\n",
        "\n",
        "# Plot 3: Jobs by source\n",
        "sources = list(stats['jobs_by_source'].keys())\n",
        "source_counts = list(stats['jobs_by_source'].values())\n",
        "axes[1, 0].pie(source_counts, labels=sources, autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 0].set_title('Jobs by Source', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 4: Jobs by language\n",
        "languages = list(stats['jobs_by_language'].keys())\n",
        "lang_counts = list(stats['jobs_by_language'].values())\n",
        "axes[1, 1].pie(lang_counts, labels=languages, autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 1].set_title('Jobs by Language', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DATA_PATH, 'skill_extraction_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Visualizations created!\")\n",
        "print(f\"   Saved to: {os.path.join(DATA_PATH, 'skill_extraction_analysis.png')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. Qualitative Evaluation\n",
        "\n",
        "**Objective:** Manually inspect a sample of extraction results to assess quality.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Quantitative metrics don't tell the full story. I need to see if the extracted skills are actually correct and useful.\n",
        "* **The Fix:** Display sample results for manual inspection.\n",
        "\n",
        "**What I did:**\n",
        "Display random samples from the extraction results with full details for quality assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- QUALITATIVE EVALUATION ---\n",
        "# Display sample extraction results for manual quality assessment\n",
        "\n",
        "import random\n",
        "\n",
        "print(\"üîç QUALITATIVE EVALUATION - Sample Extraction Results\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select 3 random samples\n",
        "samples = random.sample(extraction_results, min(3, len(extraction_results)))\n",
        "\n",
        "for i, sample in enumerate(samples, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"SAMPLE {i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Job ID: {sample['job_id']}\")\n",
        "    print(f\"Source: {sample['source']}\")\n",
        "    print(f\"Language: {sample['language']}\")\n",
        "    if sample.get('title'):\n",
        "        print(f\"Title: {sample['title']}\")\n",
        "    if sample.get('company'):\n",
        "        print(f\"Company: {sample['company']}\")\n",
        "    print(f\"Number of skill sections found by BERT: {sample['num_skill_sections']}\")\n",
        "    print(f\"Total skills extracted: {sample['total_skills_count']}\")\n",
        "    \n",
        "    print(f\"\\nüìã Extracted Skills:\")\n",
        "    for category, skills in sample['extracted_skills'].items():\n",
        "        if skills:\n",
        "            print(f\"\\n  {category.replace('_', ' ').upper()}:\")\n",
        "            for skill in sorted(skills):\n",
        "                print(f\"    ‚Ä¢ {skill}\")\n",
        "    \n",
        "    if sample['total_skills_count'] == 0:\n",
        "        print(\"\\n  ‚ö†Ô∏è No skills extracted for this job\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nüí° Quality Assessment Questions:\")\n",
        "print(\"  1. Are the extracted skills actually mentioned in the job ads?\")\n",
        "print(\"  2. Are job titles incorrectly classified as skills?\")\n",
        "print(\"  3. Are skills properly categorized?\")\n",
        "print(\"  4. Are there obvious skills that were missed?\")\n",
        "print(\"  5. Are the skill names properly normalized?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13. Save Results to Google Drive\n",
        "\n",
        "**Objective:** Backup all extraction results and analyses to Google Drive for permanent storage.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Google Colab sessions are temporary - all files will be lost when the session ends.\n",
        "* **The Fix:** Copy results to Google Drive for permanent storage and access.\n",
        "\n",
        "**What I did:**\n",
        "1. Create a backup folder in Google Drive\n",
        "2. Copy all result files (JSON, images, logs)\n",
        "3. Verify successful backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- SAVE TO GOOGLE DRIVE ---\n",
        "# Backup all results to Google Drive\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define Google Drive save path\n",
        "drive_save_path = '/content/drive/MyDrive/GEN03/skill_extraction_results'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "print(f\"üíæ Backing up results to Google Drive...\")\n",
        "print(f\"   Destination: {drive_save_path}\\n\")\n",
        "\n",
        "# List of files to backup\n",
        "files_to_backup = [\n",
        "    ('extracted_skills.json', 'Main extraction results'),\n",
        "    ('skill_statistics.json', 'Summary statistics'),\n",
        "    ('extraction_errors.json', 'Error log'),\n",
        "    ('skill_extraction_analysis.png', 'Analysis visualization')\n",
        "]\n",
        "\n",
        "# Copy files\n",
        "backed_up = 0\n",
        "for filename, description in files_to_backup:\n",
        "    source_path = os.path.join(DATA_PATH, filename)\n",
        "    dest_path = os.path.join(drive_save_path, filename)\n",
        "    \n",
        "    if os.path.exists(source_path):\n",
        "        shutil.copy(source_path, dest_path)\n",
        "        file_size = os.path.getsize(source_path) / 1024  # Size in KB\n",
        "        print(f\"‚úÖ {filename}\")\n",
        "        print(f\"   ({description}, {file_size:.1f} KB)\")\n",
        "        backed_up += 1\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  {filename} not found (skipped)\")\n",
        "\n",
        "print(f\"\\nüéâ Backup complete!\")\n",
        "print(f\"   {backed_up}/{len(files_to_backup)} files backed up successfully\")\n",
        "print(f\"   Location: {drive_save_path}\")\n",
        "print(\"\\n‚úÖ You can now safely close this notebook. All data is saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 14. Summary and Next Steps\n",
        "\n",
        "**üéâ Task 5 Complete!**\n",
        "\n",
        "### What We Accomplished:\n",
        "1. ‚úÖ Loaded the fine-tuned BERT model for zone identification\n",
        "2. ‚úÖ Implemented two-stage extraction pipeline (BERT + LLM)\n",
        "3. ‚úÖ Processed both annotated and scraped job advertisements\n",
        "4. ‚úÖ Extracted and categorized professional skills\n",
        "5. ‚úÖ Generated comprehensive statistics and visualizations\n",
        "6. ‚úÖ Saved all results to permanent storage\n",
        "\n",
        "### Key Outputs:\n",
        "- **extracted_skills.json** - Complete extraction results for all jobs\n",
        "- **skill_statistics.json** - Summary statistics and metrics\n",
        "- **skill_extraction_analysis.png** - Visual analysis of results\n",
        "- **extraction_errors.json** - Log of any errors encountered\n",
        "\n",
        "### Methodology:\n",
        "1. **Zone Identification (BERT)**: Used fine-tuned multilingual BERT to identify text sections labeled as \"F√§higkeiten und Inhalte\" (Skills and Content)\n",
        "2. **Skill Extraction (LLM)**: Used LLM (OpenAI GPT-4o-mini or Google Gemini 1.5 Flash) with carefully crafted prompts to extract individual skills from identified sections\n",
        "3. **Categorization**: Organized skills into 6 categories: technical skills, soft skills, languages, tools, certifications, and other skills\n",
        "4. **Dual LLM Support**: Implemented support for both OpenAI and Gemini APIs, allowing comparison and choice of best model\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
