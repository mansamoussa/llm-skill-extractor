{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansamoussa/llm-skill-extractor/blob/main/notebooks/01_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e8c78d",
      "metadata": {
        "id": "01e8c78d"
      },
      "source": [
        "# Multi-Stage Job Advertisement Analysis ‚Äî Data Preparation\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mansamoussa/llm-skill-extractor/blob/main/notebooks/01_data_preparation.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "### Objective\n",
        "Prepare the raw annotated dataset (`annotated.json`) for training a multilingual BERT model for **zone identification** in job advertisements.\n",
        "\n",
        "This notebook will:\n",
        "1. Load and parse the annotated dataset  \n",
        "2. Tokenize text using `BertTokenizerFast`  \n",
        "3. Align character-level labels with tokens  \n",
        "4. Handle long sequences using the sliding window approach  \n",
        "5. Generate and save:\n",
        "   - `label2id.json`  \n",
        "   - `id2label.json`  \n",
        "   - PyTorch `train_dataset` and `test_dataset`\n",
        "\n",
        "### Input Data\n",
        "- `data/annotated.json` ‚Äî cleaned and annotated job ads  \n",
        "- `src/preprocessing.py` ‚Äî preprocessing helper script  \n",
        "\n",
        "### Output\n",
        "- Tokenized and labeled datasets  \n",
        "- `label2id.json` and `id2label.json` mapping files  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. Setting up my Environment\n",
        "\n",
        "**Objective:** Prepare the Google Colab environment by cloning the project repository and installing the necessary Python dependencies.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** When I start Google Colab, it is empty. It does not have my project files.\n",
        "* **The Fix:** I clone my repository. This makes sure I have my code (like `preprocessing.py`). If I do not do this, I will get errors about missing files.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Clone Repository:** I downloaded my files from GitHub.\n",
        "2.  **Install Dependencies:** I installed the Python libraries I need using`requirements.txt`."
      ],
      "": {
        "id": "j5OxFbMmHciq"
      },
      "id": "j5OxFbMmHciq"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP STEP ---\n",
        "# This cell prepares the Colab environment by downloading the code and installing libraries.\n",
        "import os\n",
        "\n",
        "# 1. Clone the repository if it doesn't exist in the notebook\n",
        "if not os.path.exists('llm-skill-extractor'):\n",
        "    !git clone https://github.com/mansamoussa/llm-skill-extractor.git\n",
        "else:\n",
        "    print(\"Repository already cloned.\")\n",
        "\n",
        "# 2. Install dependencies\n",
        "!pip install -r llm-skill-extractor/requirements.txt"
      ],
      "metadata": {
        "id": "ybpvA89pHh-t",
        "outputId": "9aae8dce-8324-414a-c42d-c808cc21e95c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ybpvA89pHh-t",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-skill-extractor'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 32 (delta 9), reused 21 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 2.83 MiB | 7.44 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading my Data\n",
        "\n",
        "**Objective:** Securely import the raw `annotated.json` dataset from Google Drive into the local runtime environment.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** My data file is private. It is not on GitHub. If I skip this, the code will crash because the `data/` folder is empty.\n",
        "* **The Fix:** I connect to my Google Drive. I copy the file from Drive to my project folder manually.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Mount Drive:** I connected my Google Drive.\n",
        "2.  **Copy Data:** I copied `annotated.json` to the `data/` folder."
      ],
      "metadata": {
        "id": "QlL0N7KgHpkr"
      },
      "id": "QlL0N7KgHpkr"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DATA LOADING STEP ---\n",
        "# This cell brings the gridve data into the project environment\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define paths (here works with mika's gdrive)\n",
        "source_path = '/content/drive/MyDrive/GEN03/annotated.json'\n",
        "destination_folder = '/content/llm-skill-extractor/data/'\n",
        "\n",
        "# 3. Copy the file\n",
        "if os.path.exists(source_path):\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "    shutil.copy(source_path, destination_folder)\n",
        "    print(f\"‚úÖ Success! Data copied to {destination_folder}\")\n",
        "else:\n",
        "    print(f\"‚ùå File not found at {source_path}\")\n",
        "    print(\"Please verify the path in your Drive or upload 'annotated.json' manually to the 'llm-skill-extractor/data/' folder using the Files sidebar on the left.\")"
      ],
      "metadata": {
        "id": "_7XRNV_gHyq3",
        "outputId": "f3010d0e-40f5-437a-fb1a-9aca81cb1065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_7XRNV_gHyq3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Success! Data copied to /content/llm-skill-extractor/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Configuring my Project\n",
        "\n",
        "**Objective:** Configure the Python execution environment to recognize custom modules and define all required input/output file paths.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue 1 (Logging File):** The `preprocessing.py` script attempts to save a log file using a relative path (`../data/preprocessing.log`). Because I was running the notebook from the root folder, the script failed with a `FileNotFoundError` during import.\n",
        "* **The Fix 1:** I temporarily changed the working directory to the `src` folder just long enough for the script to set up its logging correctly, and then immediately switched back.\n",
        "* **The Issue 2 (Silent Logs):** The script also removes all existing log handlers, silencing all output in the notebook.\n",
        "* **The Fix 2:** I explicitly added the console logger back *after* the import so I can see the processing messages.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Directory Fix:** I temporarily changed the directory to `src/` to resolve the log file path issue.\n",
        "2.  **System Path:** I added the `src/` folder to the system path.\n",
        "3.  **Imports:** I imported my custom functions (`preprocess_data`, `create_dataset`).\n",
        "4.  **Logging:** I restored the console logging (screen output).\n",
        "5.  **Paths:** I defined all project file paths."
      ],
      "metadata": {
        "id": "wiD2h1WSJAJE"
      },
      "id": "wiD2h1WSJAJE"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Step ---\n",
        "# This cell links the environment to the custom scripts in the 'src' folder.\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Define Project Structure\n",
        "PROJECT_ROOT = '/content/llm-skill-extractor'\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, 'src')\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n",
        "\n",
        "# 2. Ensure Python can find the code\n",
        "# We add 'src' to the system path\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.append(SRC_PATH)\n",
        "    print(f\"‚úÖ Added '{SRC_PATH}' to system path.\")\n",
        "\n",
        "# 3. Import the Module using a temporary directory change (The fix!)\n",
        "original_dir = os.getcwd() # Save where we are now\n",
        "try:\n",
        "    # Change directory so the relative log path (../data) resolves correctly\n",
        "    os.chdir(SRC_PATH)\n",
        "    print(f\"üîÑ Temporarily changed directory to: {os.getcwd()}\")\n",
        "\n",
        "    # Importing 'preprocessing' will now execute the logging setup successfully\n",
        "    from preprocessing import preprocess_data, create_dataset\n",
        "    print(\"‚úÖ 'preprocessing' module imported successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CRITICAL ERROR during import: {e}\")\n",
        "    raise e\n",
        "\n",
        "finally:\n",
        "    # Always change the directory back immediately\n",
        "    os.chdir(original_dir)\n",
        "    print(f\"‚û°Ô∏è Directory restored to: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# 4. Configure Logging\n",
        "# The preprocessing script removes screen logs. We turn them back on here.\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "has_screen_handler = any(isinstance(h, logging.StreamHandler) for h in logger.handlers)\n",
        "if not has_screen_handler:\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "    logger.addHandler(console_handler)\n",
        "    print(\"‚úÖ Logger output restored to screen.\")\n",
        "\n",
        "# 5. Define File Paths for the next steps\n",
        "INPUT_FILE_PATH = os.path.join(DATA_PATH, 'annotated.json')\n",
        "LABEL_MAPPING_PATH = os.path.join(PROJECT_ROOT, 'model', 'label2id.json')\n",
        "ID2LABEL_PATH = os.path.join(PROJECT_ROOT, 'model', 'id2label.json')\n",
        "TRAIN_DATASET_PATH = os.path.join(DATA_PATH, 'train_dataset.pt')\n",
        "TEST_DATASET_PATH = os.path.join(DATA_PATH, 'test_dataset.pt')\n",
        "\n",
        "print(\"\\nüöÄ SETUP COMPLETE. You can now run the 'Load Data' cell.\")"
      ],
      "metadata": {
        "id": "sloLRodEJCGj",
        "outputId": "870cf597-44bf-48e9-aa6e-6adaa3fcfa24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sloLRodEJCGj",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Added '/content/llm-skill-extractor/src' to system path.\n",
            "üîÑ Temporarily changed directory to: /content/llm-skill-extractor/src\n",
            "‚úÖ 'preprocessing' module imported successfully!\n",
            "‚û°Ô∏è Directory restored to: /content\n",
            "\n",
            "üöÄ SETUP COMPLETE. You can now run the 'Load Data' cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Reading the Data\n",
        "\n",
        "**Objective:** Load the raw JSON data into a Pandas DataFrame and perform initial data quality filtering.\n",
        "\n",
        "**What I am doing:** I am reading my raw data into a table so I can look at it.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Load JSON:** I read the `annotated.json` file.\n",
        "2.  **Filter:** I removed empty rows. This stops errors from happening later."
      ],
      "metadata": {
        "id": "tQiYR-4_JHY4"
      },
      "id": "tQiYR-4_JHY4"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEP 1: Load and parse the annotated dataset\n",
        "logger.info(f\"üìñ Reading data from {INPUT_FILE_PATH}...\")\n",
        "\n",
        "try:\n",
        "    # Load JSON into a Pandas DataFrame\n",
        "    df = pd.read_json(INPUT_FILE_PATH)\n",
        "\n",
        "    # Filter out rows that don't have valid annotations (prevent errors later)\n",
        "    initial_count = len(df)\n",
        "    df = df[df.annotations.apply(lambda x: isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict) and len(x[0].get('result', [])) > 0)]\n",
        "\n",
        "    logger.info(f\"‚úÖ Successfully loaded {len(df)} rows (filtered from {initial_count}).\")\n",
        "\n",
        "    # Display the first few rows to check\n",
        "    display(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Error loading data: {e}\")"
      ],
      "metadata": {
        "id": "hkt7skF8JJMq",
        "outputId": "ecd29618-045f-4568-a94a-7639ae05493f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "id": "hkt7skF8JJMq",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id                                        annotations  \\\n",
              "0   1  [{'id': 10, 'completed_by': 2, 'result': [{'va...   \n",
              "1   2  [{'id': 7, 'completed_by': 4, 'result': [{'val...   \n",
              "2   3  [{'id': 11, 'completed_by': 2, 'result': [{'va...   \n",
              "3   4  [{'id': 12, 'completed_by': 4, 'result': [{'va...   \n",
              "4   5  [{'id': 13, 'completed_by': 2, 'result': [{'va...   \n",
              "\n",
              "                                file_upload  \\\n",
              "0  e6d47862-240625_content_clean_Kopie.json   \n",
              "1  e6d47862-240625_content_clean_Kopie.json   \n",
              "2  e6d47862-240625_content_clean_Kopie.json   \n",
              "3  e6d47862-240625_content_clean_Kopie.json   \n",
              "4  e6d47862-240625_content_clean_Kopie.json   \n",
              "\n",
              "                                              drafts predictions  \\\n",
              "0                                                 []          []   \n",
              "1                                                 []          []   \n",
              "2                                                 []          []   \n",
              "3                                                 []          []   \n",
              "4  [{'id': 1049, 'user': 'marcel.blattner@x28.ch'...          []   \n",
              "\n",
              "                                                data meta  \\\n",
              "0  {'duplicate_group': '50481aee-11dd-4167-bc63-8...   {}   \n",
              "1  {'duplicate_group': '609e314f-6ed2-4876-beb2-0...   {}   \n",
              "2  {'duplicate_group': 'ed0ea7e0-253f-40a9-8e09-4...   {}   \n",
              "3  {'duplicate_group': '5cd13b12-9c04-4477-9dc7-8...   {}   \n",
              "4  {'duplicate_group': '0fa3d5dd-eaa6-4134-bad9-2...   {}   \n",
              "\n",
              "                        created_at                       updated_at  inner_id  \\\n",
              "0 2024-06-25 11:28:24.727849+00:00 2024-06-25 11:50:28.913292+00:00         1   \n",
              "1 2024-06-25 11:28:24.727981+00:00 2024-06-25 11:44:47.858773+00:00         2   \n",
              "2 2024-06-25 11:28:24.728044+00:00 2024-06-25 11:55:00.450712+00:00         3   \n",
              "3 2024-06-25 11:28:24.728132+00:00 2024-06-25 11:56:05.915975+00:00         4   \n",
              "4 2024-06-25 11:28:24.728189+00:00 2024-06-25 11:57:00.795412+00:00         5   \n",
              "\n",
              "   total_annotations  cancelled_annotations  total_predictions  comment_count  \\\n",
              "0                  1                      0                  0              0   \n",
              "1                  1                      0                  0              0   \n",
              "2                  1                      0                  0              0   \n",
              "3                  1                      0                  0              0   \n",
              "4                  1                      0                  0              0   \n",
              "\n",
              "   unresolved_comment_count last_comment_updated_at  project  updated_by  \\\n",
              "0                         0                     NaT        1           2   \n",
              "1                         0                     NaT        1           4   \n",
              "2                         0                     NaT        1           2   \n",
              "3                         0                     NaT        1           4   \n",
              "4                         0                     NaT        1           2   \n",
              "\n",
              "  comment_authors  \n",
              "0              []  \n",
              "1              []  \n",
              "2              []  \n",
              "3              []  \n",
              "4              []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b078cd3-3093-4db1-91f3-f295caa171f0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>annotations</th>\n",
              "      <th>file_upload</th>\n",
              "      <th>drafts</th>\n",
              "      <th>predictions</th>\n",
              "      <th>data</th>\n",
              "      <th>meta</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>inner_id</th>\n",
              "      <th>total_annotations</th>\n",
              "      <th>cancelled_annotations</th>\n",
              "      <th>total_predictions</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>unresolved_comment_count</th>\n",
              "      <th>last_comment_updated_at</th>\n",
              "      <th>project</th>\n",
              "      <th>updated_by</th>\n",
              "      <th>comment_authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[{'id': 10, 'completed_by': 2, 'result': [{'va...</td>\n",
              "      <td>e6d47862-240625_content_clean_Kopie.json</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'duplicate_group': '50481aee-11dd-4167-bc63-8...</td>\n",
              "      <td>{}</td>\n",
              "      <td>2024-06-25 11:28:24.727849+00:00</td>\n",
              "      <td>2024-06-25 11:50:28.913292+00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaT</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>[{'id': 7, 'completed_by': 4, 'result': [{'val...</td>\n",
              "      <td>e6d47862-240625_content_clean_Kopie.json</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'duplicate_group': '609e314f-6ed2-4876-beb2-0...</td>\n",
              "      <td>{}</td>\n",
              "      <td>2024-06-25 11:28:24.727981+00:00</td>\n",
              "      <td>2024-06-25 11:44:47.858773+00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaT</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>[{'id': 11, 'completed_by': 2, 'result': [{'va...</td>\n",
              "      <td>e6d47862-240625_content_clean_Kopie.json</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'duplicate_group': 'ed0ea7e0-253f-40a9-8e09-4...</td>\n",
              "      <td>{}</td>\n",
              "      <td>2024-06-25 11:28:24.728044+00:00</td>\n",
              "      <td>2024-06-25 11:55:00.450712+00:00</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaT</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>[{'id': 12, 'completed_by': 4, 'result': [{'va...</td>\n",
              "      <td>e6d47862-240625_content_clean_Kopie.json</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'duplicate_group': '5cd13b12-9c04-4477-9dc7-8...</td>\n",
              "      <td>{}</td>\n",
              "      <td>2024-06-25 11:28:24.728132+00:00</td>\n",
              "      <td>2024-06-25 11:56:05.915975+00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaT</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>[{'id': 13, 'completed_by': 2, 'result': [{'va...</td>\n",
              "      <td>e6d47862-240625_content_clean_Kopie.json</td>\n",
              "      <td>[{'id': 1049, 'user': 'marcel.blattner@x28.ch'...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'duplicate_group': '0fa3d5dd-eaa6-4134-bad9-2...</td>\n",
              "      <td>{}</td>\n",
              "      <td>2024-06-25 11:28:24.728189+00:00</td>\n",
              "      <td>2024-06-25 11:57:00.795412+00:00</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaT</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b078cd3-3093-4db1-91f3-f295caa171f0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b078cd3-3093-4db1-91f3-f295caa171f0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b078cd3-3093-4db1-91f3-f295caa171f0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c5dac452-0855-44aa-aac4-84d3c6348e93\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c5dac452-0855-44aa-aac4-84d3c6348e93')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c5dac452-0855-44aa-aac4-84d3c6348e93 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    logger\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotations\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_upload\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"e6d47862-240625_content_clean_Kopie.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drafts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predictions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"meta\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-06-25 11:28:24.727849+00:00\",\n        \"max\": \"2024-06-25 11:28:24.728189+00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-06-25 11:28:24.727981+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"updated_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-06-25 11:44:47.858773+00:00\",\n        \"max\": \"2024-06-25 11:57:00.795412+00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-06-25 11:44:47.858773+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inner_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_annotations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cancelled_annotations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_predictions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unresolved_comment_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_comment_updated_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"NaT\",\n        \"max\": \"NaT\",\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"project\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"updated_by\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Processing the Text\n",
        "\n",
        "**Objective:** Transform raw text and annotations into tokenized, BERT-compatible sequences using a sliding window approach.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** BERT cannot read very long texts (more than 512 tokens). Many of my job ads are too long.\n",
        "* **The Fix:** I used a \"Sliding Window\". This cuts the long text into smaller pieces that overlap. The code handles the warning about \"sequence length\" automatically.\n",
        "\n",
        "**What I did:**\n",
        "* I tokenized the text.\n",
        "* I split long documents into chunks.\n",
        "* I matched the labels to the correct tokens."
      ],
      "metadata": {
        "id": "DiqR8n_kJLlX"
      },
      "id": "DiqR8n_kJLlX"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEPS 2, 3, & 4:\n",
        "# - Tokenize text using BertTokenizerFast\n",
        "# - Align character-level labels with tokens\n",
        "# - Handle long sequences using the sliding window approach\n",
        "logger.info(\"‚öôÔ∏è Starting preprocessing (Tokenization & Label Alignment)...\")\n",
        "\n",
        "try:\n",
        "    # preprocess_data is the function we imported from your 'preprocessing.py' file\n",
        "    # this function performs all the steps above.\n",
        "    processed_data, label2id = preprocess_data(df)\n",
        "\n",
        "    logger.info(\"‚úÖ Preprocessing complete!\")\n",
        "    logger.info(f\"üì¶ Generated {len(processed_data)} total sequences (chunks).\")\n",
        "    logger.info(f\"üè∑Ô∏è  Labels found: {list(label2id.keys())}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Error during preprocessing: {e}\")"
      ],
      "metadata": {
        "id": "pXi6gXmTJPt2",
        "outputId": "5efa4fdc-56d7-48b3-a753-911cdd266650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "45844a7a2945468eb61205ab889038ab",
            "7c2819992c4644a1842d20e12aba40a9",
            "4a848b67f431494abda5766a545c2f61",
            "974c2eda923a4dda9bd87cb354a8b835",
            "92b4d318d9dc43f9b7c79d206aa2ed0e",
            "d7005e6c3d074b9b9562c755c4e3186f",
            "98cf9ace819d42769f928bf126a456c4",
            "9757b6668a9c4d5d901828043ac6e45f",
            "a2477498751e4b67804cd85c351964d6",
            "eff56d55b0c44393bf14c9fdccaf302f",
            "c780a38a24554796b23cf0beb695d496",
            "7beb09b5b4154fbc8169fb1f9132b341",
            "35227de22e874d92a90a63379f64cee8",
            "f0cd7b6ba47a44f696dae48594c8115f",
            "a6525607a61049b99b2684c70bf86114",
            "ef03893a51cc4918aae03d54bb976684",
            "4a630b1107854243bfdfbc5d6e226c76",
            "bb87f5b97200410cb39c8d735f00adff",
            "1a2e536351244e12883617099adbf007",
            "5800e0235f2f45379ab903df3fdd402f",
            "e0d798b67d5e48e2beadeebe9ae80922",
            "b11330906aaf42d9811818dfc01f595e",
            "e6ab698697194d8cad6fd91c39c451b8",
            "81b77315a5ce400a8250fb3d37360184",
            "3bac7addc64847638b4bf6e416d44318",
            "fa848b9f155448b88abf098dc09667f4",
            "b8de29b9d38341238fa039dc1553d4cf",
            "7b1f462a03be4427a0ae7b5c79941f1e",
            "e1148aa012064bc395fdab448c8fe9cb",
            "2bb3d2e72eb4488d9c5968494bf661e8",
            "9f448db087574822808084c49578fb18",
            "1b7d31094f6b4549bcfdb7ee8580eb34",
            "71b2aa6b0c144a5282c2512c4f84657a",
            "7b328e535c6e4a2c83f4b2db96579c78",
            "2c4868ea4e384107b3655284be9be031",
            "b4a8a6d570fc42a8acdd20b2014af53b",
            "d884eda37452488aafc9b4fa712ae0d9",
            "cc3d72d81fd94bda9329744e41bf2518",
            "b55fa102e6654e8ba97824c8215d59a2",
            "e6288f25224b426faac2e2ceb3a8b6a7",
            "486b95c7249c4269be5da46b8d42ff0b",
            "c0257dd92842473a99960ae086b033ae",
            "9ae797c5d5b3417583532fd07b3d744a",
            "196c7b9487e243c1892496418d842def"
          ]
        }
      },
      "id": "pXi6gXmTJPt2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45844a7a2945468eb61205ab889038ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7beb09b5b4154fbc8169fb1f9132b341"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6ab698697194d8cad6fd91c39c451b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b328e535c6e4a2c83f4b2db96579c78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Saving the Labels\n",
        "\n",
        "**Objective:** Generate and persist the `label2id` and `id2label` mappings to ensure consistent decoding of model predictions.\n",
        "\n",
        "**Why I need this:**\n",
        "I need to know which number matches which label (e.g., 0 = \"Anstellung\"). I will need these files later to understand what the model predicts."
      ],
      "metadata": {
        "id": "-KxdjHiKJVGo"
      },
      "id": "-KxdjHiKJVGo"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEP 5 (Part A): Generate and save label2id.json and id2label.json\n",
        "import json\n",
        "\n",
        "# Create inverse mapping (ID -> Label)\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(os.path.dirname(LABEL_MAPPING_PATH), exist_ok=True)\n",
        "\n",
        "# Save label2id.json\n",
        "with open(LABEL_MAPPING_PATH, 'w') as f:\n",
        "    json.dump(label2id, f, indent=2)\n",
        "logger.info(f\"üíæ Saved label2id to: {LABEL_MAPPING_PATH}\")\n",
        "\n",
        "# Save id2label.json\n",
        "with open(ID2LABEL_PATH, 'w') as f:\n",
        "    json.dump(id2label, f, indent=2)\n",
        "logger.info(f\"üíæ Saved id2label to: {ID2LABEL_PATH}\")"
      ],
      "metadata": {
        "id": "Zf1bUKlnJW-v"
      },
      "id": "Zf1bUKlnJW-v",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Saving the Datasets\n",
        "\n",
        "**Objective:** Split the processed data into training and evaluation sets, convert them into PyTorch TensorDatasets, and save them to disk for the training phase.\n",
        "\n",
        "**What I am doing:** I am saving the final data so it is ready for training.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Split:** I separated the data: 80% for training and 20% for testing.\n",
        "2.  **Convert:** I turned the data into PyTorch format (TensorDataset).\n",
        "3.  **Save:** I saved the files to the disk (`.pt` files). Now I can load them quickly in the next notebook."
      ],
      "metadata": {
        "id": "sNx2V_CaJZ-O"
      },
      "id": "sNx2V_CaJZ-O"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEP 5 (Part B): Generate and save PyTorch train_dataset and test_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# 1. Split data into Training and Testing sets (80/20 split)\n",
        "train_data, test_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
        "logger.info(f\"Training chunks: {len(train_data)}\")\n",
        "logger.info(f\"Testing chunks:  {len(test_data)}\")\n",
        "\n",
        "# 2. Convert to PyTorch TensorDatasets\n",
        "# This handles padding so all sequences in a batch are the same length\n",
        "logger.info(\"üîÑ Converting to PyTorch Datasets...\")\n",
        "train_dataset = create_dataset(train_data, label2id)\n",
        "test_dataset = create_dataset(test_data, label2id)\n",
        "\n",
        "# 3. Save the final datasets\n",
        "logger.info(\"üíæ Saving datasets to disk...\")\n",
        "torch.save(train_dataset, TRAIN_DATASET_PATH)\n",
        "torch.save(test_dataset, TEST_DATASET_PATH)\n",
        "\n",
        "logger.info(\"üéâ Data Preparation Finished Successfully!\")\n",
        "logger.info(f\"Train Dataset saved to: {TRAIN_DATASET_PATH}\")\n",
        "logger.info(f\"Test Dataset saved to:  {TEST_DATASET_PATH}\")"
      ],
      "metadata": {
        "id": "6Miz-N_UJcHO"
      },
      "id": "6Miz-N_UJcHO",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Saving Outputs to Google Drive\n",
        "\n",
        "**Objective:** Persist the generated datasets and label mappings to permanent cloud storage to prevent data loss upon session termination.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Google Colab is temporary. If I close this tab or disconnect, all the files I just created (the datasets and maps) will be deleted immediately.\n",
        "* **The Fix:** I must copy these files to my Google Drive. This way, they are safe, and I can load them easily when I start the next notebook for training.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Create Folder:** I made a new folder in my Google Drive called `processed_data`.\n",
        "2.  **Copy Files:** I copied the 4 critical files (`train_dataset.pt`, `test_dataset.pt`, `label2id.json`, `id2label.json`) into that folder."
      ],
      "metadata": {
        "id": "Ea4v15AFJfnw"
      },
      "id": "Ea4v15AFJfnw"
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Save Outputs to Google Drive (FIXED)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Define where we want to save the results in your Drive\n",
        "# I am creating a new folder called 'processed_data' in your GEN03 folder\n",
        "drive_save_path = '/content/drive/MyDrive/GEN03/processed_data'\n",
        "\n",
        "# 2. Create the folder if it doesn't exist\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# 3. Use the ABSOLUTE PATH VARIABLES defined in Cell 3!\n",
        "# This ensures the script always finds the files, regardless of the current directory.\n",
        "files_to_save = [\n",
        "    LABEL_MAPPING_PATH,\n",
        "    ID2LABEL_PATH,\n",
        "    TRAIN_DATASET_PATH,\n",
        "    TEST_DATASET_PATH\n",
        "]\n",
        "\n",
        "# 4. Copy them\n",
        "print(f\"üöÄ Backing up files to: {drive_save_path}\")\n",
        "\n",
        "for full_source_path in files_to_save:\n",
        "    filename = os.path.basename(full_source_path)\n",
        "    destination = os.path.join(drive_save_path, filename)\n",
        "\n",
        "    if os.path.exists(full_source_path):\n",
        "        # We don't need os.path.abspath here since the variables are already absolute\n",
        "        shutil.copy(full_source_path, destination)\n",
        "        print(f\"‚úÖ Saved: {filename}\")\n",
        "    else:\n",
        "        # This should no longer happen if Cells 5, 6, and 7 ran successfully\n",
        "        print(f\"‚ö†Ô∏è Could not find: {filename} at path: {full_source_path}\")\n",
        "\n",
        "print(\"\\nüéâ Everything is saved to my Google Drive! I can safely close this tab.\")"
      ],
      "metadata": {
        "id": "1WwuiMgRJhvI",
        "outputId": "580f28e9-ae8f-4949-d60e-113d560061c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1WwuiMgRJhvI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Backing up files to: /content/drive/MyDrive/GEN03/processed_data\n",
            "‚úÖ Saved: label2id.json\n",
            "‚úÖ Saved: id2label.json\n",
            "‚úÖ Saved: train_dataset.pt\n",
            "‚úÖ Saved: test_dataset.pt\n",
            "\n",
            "üéâ Everything is saved to my Google Drive! I can safely close this tab.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }   
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
