{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01e8c78d",
      "metadata": {
        "id": "01e8c78d"
      },
      "source": [
        "# Multi-Stage Job Advertisement Analysis ‚Äî Data Preparation\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mansamoussa/llm-skill-extractor/blob/main/notebooks/01_data_preparation.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "### Objective\n",
        "Prepare the raw annotated dataset (`annotated.json`) for training a multilingual BERT model for **zone identification** in job advertisements.\n",
        "\n",
        "This notebook will:\n",
        "1. Load and parse the annotated dataset  \n",
        "2. Tokenize text using `BertTokenizerFast`  \n",
        "3. Align character-level labels with tokens  \n",
        "4. Handle long sequences using the sliding window approach  \n",
        "5. Generate and save:\n",
        "   - `label2id.json`  \n",
        "   - `id2label.json`  \n",
        "   - PyTorch `train_dataset` and `test_dataset`\n",
        "\n",
        "### Input Data\n",
        "- `data/annotated.json` ‚Äî cleaned and annotated job ads  \n",
        "- `src/preprocessing.py` ‚Äî preprocessing helper script  \n",
        "\n",
        "### Output\n",
        "- Tokenized and labeled datasets  \n",
        "- `label2id.json` and `id2label.json` mapping files  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. Setting up my Environment\n",
        "\n",
        "**Objective:** Prepare the Google Colab environment by cloning the project repository and installing the necessary Python dependencies.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** When I start Google Colab, it is empty. It does not have my project files.\n",
        "* **The Fix:** I clone my repository. This makes sure I have my code (like `preprocessing.py`). If I do not do this, I will get errors about missing files.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Clone Repository:** I downloaded my files from GitHub.\n",
        "2.  **Install Dependencies:** I installed the Python libraries I need using`requirements.txt`."
      ],
      "metadata": {
        "id": "j5OxFbMmHciq"
      },
      "id": "j5OxFbMmHciq"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP STEP ---\n",
        "# This cell prepares the Colab environment by downloading the code and installing libraries.\n",
        "import os\n",
        "\n",
        "# 1. Clone the repository if it doesn't exist in the notebook\n",
        "if not os.path.exists('llm-skill-extractor'):\n",
        "    !git clone https://github.com/mansamoussa/llm-skill-extractor.git\n",
        "else:\n",
        "    print(\"Repository already cloned.\")\n",
        "\n",
        "# 2. Install dependencies\n",
        "!pip install -r llm-skill-extractor/requirements.txt"
      ],
      "metadata": {
        "id": "ybpvA89pHh-t"
      },
      "id": "ybpvA89pHh-t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading my Data\n",
        "\n",
        "**Objective:** Securely import the raw `annotated.json` dataset from Google Drive into the local runtime environment.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** My data file is private. It is not on GitHub. If I skip this, the code will crash because the `data/` folder is empty.\n",
        "* **The Fix:** I connect to my Google Drive. I copy the file from Drive to my project folder manually.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Mount Drive:** I connected my Google Drive.\n",
        "2.  **Copy Data:** I copied `annotated.json` to the `data/` folder."
      ],
      "metadata": {
        "id": "QlL0N7KgHpkr"
      },
      "id": "QlL0N7KgHpkr"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# --- DATA LOADING STEP ---\n",
        "# This cell brings the gridve data into the project environment\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define paths (here works with mika's gdrive)\n",
        "source_path = '/content/drive/MyDrive/GEN03/annotated.json'\n",
        "destination_folder = '/content/llm-skill-extractor/data/'\n",
        "\n",
        "# 3. Copy the file\n",
        "if os.path.exists(source_path):\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "    shutil.copy(source_path, destination_folder)\n",
        "    print(f\"‚úÖ Success! Data copied to {destination_folder}\")\n",
        "else:\n",
        "    print(f\"‚ùå File not found at {source_path}\")\n",
        "    print(\"Please verify the path in your Drive or upload 'annotated.json' manually to the 'llm-skill-extractor/data/' folder using the Files sidebar on the left.\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "_7XRNV_gHyq3"
      },
      "id": "_7XRNV_gHyq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Configuring my Project\n",
        "\n",
        "**Objective:** Configure the Python execution environment to recognize custom modules and define all required input/output file paths.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue 1 (Logging File):** The `preprocessing.py` script attempts to save a log file using a relative path (`../data/preprocessing.log`). Because I was running the notebook from the root folder, the script failed with a `FileNotFoundError` during import.\n",
        "* **The Fix 1:** I temporarily changed the working directory to the `src` folder just long enough for the script to set up its logging correctly, and then immediately switched back.\n",
        "* **The Issue 2 (Silent Logs):** The script also removes all existing log handlers, silencing all output in the notebook.\n",
        "* **The Fix 2:** I explicitly added the console logger back *after* the import so I can see the processing messages.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Directory Fix:** I temporarily changed the directory to `src/` to resolve the log file path issue.\n",
        "2.  **System Path:** I added the `src/` folder to the system path.\n",
        "3.  **Imports:** I imported my custom functions (`preprocess_data`, `create_dataset`).\n",
        "4.  **Logging:** I restored the console logging (screen output).\n",
        "5.  **Paths:** I defined all project file paths."
      ],
      "metadata": {
        "id": "wiD2h1WSJAJE"
      },
      "id": "wiD2h1WSJAJE"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Step ---\n",
        "# This cell links the environment to the custom scripts in the 'src' folder.\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Define Project Structure\n",
        "PROJECT_ROOT = '/content/llm-skill-extractor'\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, 'src')\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n",
        "\n",
        "# 2. Ensure Python can find the code\n",
        "# We add 'src' to the system path\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.append(SRC_PATH)\n",
        "    print(f\"‚úÖ Added '{SRC_PATH}' to system path.\")\n",
        "\n",
        "# 3. Import the Module using a temporary directory change (The fix!)\n",
        "original_dir = os.getcwd() # Save where we are now\n",
        "try:\n",
        "    # Change directory so the relative log path (../data) resolves correctly\n",
        "    os.chdir(SRC_PATH)\n",
        "    print(f\"üîÑ Temporarily changed directory to: {os.getcwd()}\")\n",
        "\n",
        "    # Importing 'preprocessing' will now execute the logging setup successfully\n",
        "    from preprocessing import preprocess_data, create_dataset\n",
        "    print(\"‚úÖ 'preprocessing' module imported successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CRITICAL ERROR during import: {e}\")\n",
        "    raise e\n",
        "\n",
        "finally:\n",
        "    # Always change the directory back immediately\n",
        "    os.chdir(original_dir)\n",
        "    print(f\"‚û°Ô∏è Directory restored to: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# 4. Configure Logging\n",
        "# The preprocessing script removes screen logs. We turn them back on here.\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "has_screen_handler = any(isinstance(h, logging.StreamHandler) for h in logger.handlers)\n",
        "if not has_screen_handler:\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "    logger.addHandler(console_handler)\n",
        "    print(\"‚úÖ Logger output restored to screen.\")\n",
        "\n",
        "# 5. Define File Paths for the next steps\n",
        "INPUT_FILE_PATH = os.path.join(DATA_PATH, 'annotated.json')\n",
        "LABEL_MAPPING_PATH = os.path.join(PROJECT_ROOT, 'model', 'label2id.json')\n",
        "ID2LABEL_PATH = os.path.join(PROJECT_ROOT, 'model', 'id2label.json')\n",
        "TRAIN_DATASET_PATH = os.path.join(DATA_PATH, 'train_dataset.pt')\n",
        "TEST_DATASET_PATH = os.path.join(DATA_PATH, 'test_dataset.pt')\n",
        "\n",
        "print(\"\\nüöÄ SETUP COMPLETE. You can now run the 'Load Data' cell.\")"
      ],
      "metadata": {
        "id": "sloLRodEJCGj"
      },
      "id": "sloLRodEJCGj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Reading the Data\n",
        "\n",
        "**Objective:** Load the raw JSON data into a Pandas DataFrame and perform initial data quality filtering.\n",
        "\n",
        "**What I am doing:** I am reading my raw data into a table so I can look at it.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Load JSON:** I read the `annotated.json` file.\n",
        "2.  **Filter:** I removed empty rows. This stops errors from happening later."
      ],
      "metadata": {
        "id": "tQiYR-4_JHY4"
      },
      "id": "tQiYR-4_JHY4"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEP 1: Load and parse the annotated dataset\n",
        "logger.info(f\"üìñ Reading data from {INPUT_FILE_PATH}...\")\n",
        "\n",
        "try:\n",
        "    # Load JSON into a Pandas DataFrame\n",
        "    df = pd.read_json(INPUT_FILE_PATH)\n",
        "\n",
        "    # Filter out rows that don't have valid annotations (prevent errors later)\n",
        "    initial_count = len(df)\n",
        "    df = df[df.annotations.apply(lambda x: isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict) and len(x[0].get('result', [])) > 0)]\n",
        "\n",
        "    logger.info(f\"‚úÖ Successfully loaded {len(df)} rows (filtered from {initial_count}).\")\n",
        "\n",
        "    # Display the first few rows to check\n",
        "    display(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Error loading data: {e}\")"
      ],
      "metadata": {
        "id": "hkt7skF8JJMq"
      },
      "id": "hkt7skF8JJMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Processing the Text\n",
        "\n",
        "**Objective:** Transform raw text and annotations into tokenized, BERT-compatible sequences using a sliding window approach.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** BERT cannot read very long texts (more than 512 tokens). Many of my job ads are too long.\n",
        "* **The Fix:** I used a \"Sliding Window\". This cuts the long text into smaller pieces that overlap. The code handles the warning about \"sequence length\" automatically.\n",
        "\n",
        "**What I did:**\n",
        "* I tokenized the text.\n",
        "* I split long documents into chunks.\n",
        "* I matched the labels to the correct tokens."
      ],
      "metadata": {
        "id": "DiqR8n_kJLlX"
      },
      "id": "DiqR8n_kJLlX"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEPS 2, 3, & 4:\n",
        "# - Tokenize text using BertTokenizerFast\n",
        "# - Align character-level labels with tokens\n",
        "# - Handle long sequences using the sliding window approach\n",
        "logger.info(\"‚öôÔ∏è Starting preprocessing (Tokenization & Label Alignment)...\")\n",
        "\n",
        "try:\n",
        "    # preprocess_data is the function we imported from your 'preprocessing.py' file\n",
        "    # this function performs all the steps above.\n",
        "    processed_data, label2id = preprocess_data(df)\n",
        "\n",
        "    logger.info(\"‚úÖ Preprocessing complete!\")\n",
        "    logger.info(f\"üì¶ Generated {len(processed_data)} total sequences (chunks).\")\n",
        "    logger.info(f\"üè∑Ô∏è  Labels found: {list(label2id.keys())}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Error during preprocessing: {e}\")"
      ],
      "metadata": {
        "id": "pXi6gXmTJPt2"
      },
      "id": "pXi6gXmTJPt2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Saving the Labels\n",
        "\n",
        "**Objective:** Generate and persist the `label2id` and `id2label` mappings to ensure consistent decoding of model predictions.\n",
        "\n",
        "**Why I need this:**\n",
        "I need to know which number matches which label (e.g., 0 = \"Anstellung\"). I will need these files later to understand what the model predicts."
      ],
      "metadata": {
        "id": "-KxdjHiKJVGo"
      },
      "id": "-KxdjHiKJVGo"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEP 5 (Part A): Generate and save label2id.json and id2label.json\n",
        "import json\n",
        "\n",
        "# Create inverse mapping (ID -> Label)\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(os.path.dirname(LABEL_MAPPING_PATH), exist_ok=True)\n",
        "\n",
        "# Save label2id.json\n",
        "with open(LABEL_MAPPING_PATH, 'w') as f:\n",
        "    json.dump(label2id, f, indent=2)\n",
        "logger.info(f\"üíæ Saved label2id to: {LABEL_MAPPING_PATH}\")\n",
        "\n",
        "# Save id2label.json\n",
        "with open(ID2LABEL_PATH, 'w') as f:\n",
        "    json.dump(id2label, f, indent=2)\n",
        "logger.info(f\"üíæ Saved id2label to: {ID2LABEL_PATH}\")"
      ],
      "metadata": {
        "id": "Zf1bUKlnJW-v"
      },
      "id": "Zf1bUKlnJW-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Saving the Datasets\n",
        "\n",
        "**Objective:** Split the processed data into training and evaluation sets, convert them into PyTorch TensorDatasets, and save them to disk for the training phase.\n",
        "\n",
        "**What I am doing:** I am saving the final data so it is ready for training.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Split:** I separated the data: 80% for training and 20% for testing.\n",
        "2.  **Convert:** I turned the data into PyTorch format (TensorDataset).\n",
        "3.  **Save:** I saved the files to the disk (`.pt` files). Now I can load them quickly in the next notebook."
      ],
      "metadata": {
        "id": "sNx2V_CaJZ-O"
      },
      "id": "sNx2V_CaJZ-O"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT STEP 5 (Part B): Generate and save PyTorch train_dataset and test_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# 1. Split data into Training and Testing sets (80/20 split)\n",
        "train_data, test_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
        "logger.info(f\"Training chunks: {len(train_data)}\")\n",
        "logger.info(f\"Testing chunks:  {len(test_data)}\")\n",
        "\n",
        "# 2. Convert to PyTorch TensorDatasets\n",
        "# This handles padding so all sequences in a batch are the same length\n",
        "logger.info(\"üîÑ Converting to PyTorch Datasets...\")\n",
        "train_dataset = create_dataset(train_data, label2id)\n",
        "test_dataset = create_dataset(test_data, label2id)\n",
        "\n",
        "# 3. Save the final datasets\n",
        "logger.info(\"üíæ Saving datasets to disk...\")\n",
        "torch.save(train_dataset, TRAIN_DATASET_PATH)\n",
        "torch.save(test_dataset, TEST_DATASET_PATH)\n",
        "\n",
        "logger.info(\"üéâ Data Preparation Finished Successfully!\")\n",
        "logger.info(f\"Train Dataset saved to: {TRAIN_DATASET_PATH}\")\n",
        "logger.info(f\"Test Dataset saved to:  {TEST_DATASET_PATH}\")"
      ],
      "metadata": {
        "id": "6Miz-N_UJcHO"
      },
      "id": "6Miz-N_UJcHO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Saving Outputs to Google Drive\n",
        "\n",
        "**Objective:** Persist the generated datasets and label mappings to permanent cloud storage to prevent data loss upon session termination.\n",
        "\n",
        "**Why I need this:**\n",
        "* **The Issue:** Google Colab is temporary. If I close this tab or disconnect, all the files I just created (the datasets and maps) will be deleted immediately.\n",
        "* **The Fix:** I must copy these files to my Google Drive. This way, they are safe, and I can load them easily when I start the next notebook for training.\n",
        "\n",
        "**What I did:**\n",
        "1.  **Create Folder:** I made a new folder in my Google Drive called `processed_data`.\n",
        "2.  **Copy Files:** I copied the 4 critical files (`train_dataset.pt`, `test_dataset.pt`, `label2id.json`, `id2label.json`) into that folder."
      ],
      "metadata": {
        "id": "Ea4v15AFJfnw"
      },
      "id": "Ea4v15AFJfnw"
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Save Outputs to Google Drive (FIXED)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Define where we want to save the results in your Drive\n",
        "# I am creating a new folder called 'processed_data' in your GEN03 folder\n",
        "drive_save_path = '/content/drive/MyDrive/GEN03/processed_data'\n",
        "\n",
        "# 2. Create the folder if it doesn't exist\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# 3. Use the ABSOLUTE PATH VARIABLES defined in Cell 3!\n",
        "# This ensures the script always finds the files, regardless of the current directory.\n",
        "files_to_save = [\n",
        "    LABEL_MAPPING_PATH,\n",
        "    ID2LABEL_PATH,\n",
        "    TRAIN_DATASET_PATH,\n",
        "    TEST_DATASET_PATH\n",
        "]\n",
        "\n",
        "# 4. Copy them\n",
        "print(f\"üöÄ Backing up files to: {drive_save_path}\")\n",
        "\n",
        "for full_source_path in files_to_save:\n",
        "    filename = os.path.basename(full_source_path)\n",
        "    destination = os.path.join(drive_save_path, filename)\n",
        "\n",
        "    if os.path.exists(full_source_path):\n",
        "        # We don't need os.path.abspath here since the variables are already absolute\n",
        "        shutil.copy(full_source_path, destination)\n",
        "        print(f\"‚úÖ Saved: {filename}\")\n",
        "    else:\n",
        "        # This should no longer happen if Cells 5, 6, and 7 ran successfully\n",
        "        print(f\"‚ö†Ô∏è Could not find: {filename} at path: {full_source_path}\")\n",
        "\n",
        "print(\"\\nüéâ Everything is saved to my Google Drive! I can safely close this tab.\")"
      ],
      "metadata": {
        "id": "1WwuiMgRJhvI"
      },
      "id": "1WwuiMgRJhvI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}